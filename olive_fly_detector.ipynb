{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906a02f3-1371-4b2c-9768-29dca69ac5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For image processing\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "\n",
    "# For progress bar\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea9a9d6-b936-40a7-ba63-ac43e9840383",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Feature extractor\"\"\"\n",
    "    def __init__(self, target_size=(64, 64)):\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def extract_shape_features(self, img):\n",
    "        \"\"\"Extract shape-based features\"\"\"\n",
    "        # Find contours\n",
    "        thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if contours:\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            # Shape features\n",
    "            area = cv2.contourArea(largest_contour)\n",
    "            perimeter = cv2.arcLength(largest_contour, True)\n",
    "            circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0\n",
    "            \n",
    "            # Bounding box features\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "            aspect_ratio = float(w)/h if h > 0 else 0\n",
    "            extent = float(area)/(w*h) if w*h > 0 else 0\n",
    "            \n",
    "            return [area, perimeter, circularity, aspect_ratio, extent]\n",
    "        return [0, 0, 0, 0, 0]\n",
    "    \n",
    "    def extract_texture_features(self, img):\n",
    "        \"\"\"Extract texture-based features\"\"\"\n",
    "        # Gray-Level Co-occurrence Matrix\n",
    "        glcm = self.calculate_glcm(img)\n",
    "        contrast = np.sum(((np.arange(256)[:, None] - np.arange(256)) ** 2) * glcm)\n",
    "        correlation = np.sum(glcm * np.outer(np.arange(256), np.arange(256)))\n",
    "        energy = np.sum(glcm ** 2)\n",
    "        homogeneity = np.sum(glcm / (1 + (np.arange(256)[:, None] - np.arange(256)) ** 2))\n",
    "        \n",
    "        return [contrast, correlation, energy, homogeneity]\n",
    "    \n",
    "    def calculate_glcm(self, img):\n",
    "        \"\"\"Calculate Gray-Level Co-occurrence Matrix\"\"\"\n",
    "        glcm = np.zeros((256, 256))\n",
    "        for i in range(img.shape[0]-1):\n",
    "            for j in range(img.shape[1]-1):\n",
    "                i_val = img[i,j]\n",
    "                j_val = img[i,j+1]\n",
    "                glcm[i_val,j_val] += 1\n",
    "        glcm = glcm / np.sum(glcm)\n",
    "        return glcm\n",
    "    \n",
    "    def extract_intensity_features(self, img):\n",
    "        \"\"\"Extract intensity-based features\"\"\"\n",
    "        hist = cv2.calcHist([img], [0], None, [32], [0, 256]).flatten()\n",
    "        hist = hist / np.sum(hist)  # Normalize\n",
    "        \n",
    "        return [\n",
    "            np.mean(img),           # Mean intensity\n",
    "            np.std(img),            # Standard deviation\n",
    "            np.percentile(img, 25), # First quartile\n",
    "            np.percentile(img, 75), # Third quartile\n",
    "            np.max(img),            # Maximum intensity\n",
    "            np.min(img),            # Minimum intensity\n",
    "            np.median(img),         # Median intensity\n",
    "            *hist                   # Histogram\n",
    "        ]\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features_list = []\n",
    "        \n",
    "        for img in tqdm(X, desc=\"Extracting features\"):\n",
    "            # Resize and convert to grayscale\n",
    "            if img.shape[:2] != self.target_size:\n",
    "                img = cv2.resize(img, self.target_size)\n",
    "            if len(img.shape) > 2:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # HOG features\n",
    "            hog_feat = hog(img, orientations=8, pixels_per_cell=(16, 16),\n",
    "                               cells_per_block=(1, 1), visualize=False)\n",
    "            \n",
    "            # Shape features\n",
    "            shape_features = self.extract_shape_features(img)\n",
    "            \n",
    "            # Texture features\n",
    "            texture_features = self.extract_texture_features(img)\n",
    "            \n",
    "            # Intensity features\n",
    "            intensity_features = self.extract_intensity_features(img)\n",
    "            \n",
    "            # Combine all features\n",
    "            combined_features = np.concatenate([\n",
    "                hog_feat,\n",
    "                shape_features,\n",
    "                texture_features,\n",
    "                intensity_features\n",
    "            ])\n",
    "            \n",
    "            features_list.append(combined_features)\n",
    "        \n",
    "        return np.array(features_list)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8468de20-4b42-4045-bf76-e37ec29847b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OliveFlyDetector:\n",
    "    def __init__(self, n_trees=100, max_depth=10):\n",
    "        \"\"\"Initialize detector with a scikit-learn pipeline\"\"\"\n",
    "        self.pipeline = Pipeline([\n",
    "            ('feature_extractor', ImageFeatureExtractor()),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', RandomForestClassifier(\n",
    "                n_estimators=n_trees,\n",
    "                max_depth=max_depth,\n",
    "                n_jobs=1,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # Split data for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42 # 42 is a popular random Int seed\n",
    "        )\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Display validation results\n",
    "        y_pred = self.pipeline.predict(X_val)\n",
    "        print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "        print(\"\\nModel Performance:\")\n",
    "        print(classification_report(y_val, y_pred))\n",
    "        \n",
    "    def predict_batch(self, images, image_paths):\n",
    "        \"\"\"Predict multiple images with progress bar\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for img, path in tqdm(zip(images, image_paths), total=len(images), desc=\"Processing images\"):\n",
    "            prediction, probability = self.predict(img)\n",
    "            results.append({\n",
    "                'path': path,\n",
    "                'prediction': 'Olive Fly' if prediction else 'Not Olive Fly',\n",
    "                'confidence': probability[1] if prediction else probability[0]\n",
    "            })\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def predict(self, image):\n",
    "        \"\"\"Predict single image\"\"\"\n",
    "        X = np.array([image])\n",
    "        prediction = self.pipeline.predict(X)[0]\n",
    "        probability = self.pipeline.predict_proba(X)[0]\n",
    "        return prediction, probability\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save pipeline\"\"\"\n",
    "        joblib.dump(self.pipeline, filepath)\n",
    "    \n",
    "    # With this annotation the method will be bound to the class itself and not the instance of the class\n",
    "    @classmethod\n",
    "    def load_model(cls, filepath):\n",
    "        \"\"\"Load pipeline\"\"\"\n",
    "        instance = cls()\n",
    "        instance.pipeline = joblib.load(filepath)\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9180eebb-1d15-4a24-94e6-02b8f51dd0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test images from test_images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test images: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 1501.21it/s]\n",
      "Processing images:   0%|                                                                                                          | 0/18 [00:00<?, ?it/s]\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 83.34it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 70.00it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 71.50it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 99.69it/s]\n",
      "Processing images:  11%|██████████▉                                                                                       | 2/18 [00:00<00:01, 11.58it/s]\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.49it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 66.26it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 82.57it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 82.68it/s]\n",
      "Processing images:  22%|█████████████████████▊                                                                            | 4/18 [00:00<00:01, 11.54it/s]\n",
      "Extracting features: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 100.98it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 99.89it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 90.90it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 86.75it/s]\n",
      "Processing images:  33%|████████████████████████████████▋                                                                 | 6/18 [00:00<00:00, 12.18it/s]\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 71.40it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 57.02it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 71.47it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 83.43it/s]\n",
      "Processing images:  44%|███████████████████████████████████████████▌                                                      | 8/18 [00:00<00:00, 11.21it/s]\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 83.28it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 53.87it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.12it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 83.31it/s]\n",
      "Processing images:  56%|█████████████████████████████████████████████████████▉                                           | 10/18 [00:00<00:00, 10.99it/s]\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 90.83it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.46it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.46it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 71.16it/s]\n",
      "Processing images:  67%|████████████████████████████████████████████████████████████████▋                                | 12/18 [00:01<00:00, 10.86it/s]\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 83.37it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 66.71it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.12it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 55.49it/s]\n",
      "Processing images:  78%|███████████████████████████████████████████████████████████████████████████▍                     | 14/18 [00:01<00:00, 10.56it/s]\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 76.95it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 47.58it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.52it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 66.48it/s]\n",
      "Processing images:  89%|██████████████████████████████████████████████████████████████████████████████████████▏          | 16/18 [00:01<00:00, 10.27it/s]\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 56.99it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 66.04it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.46it/s]\n",
      "\n",
      "Extracting features: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 46.17it/s]\n",
      "Processing images: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:01<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "--------------------------------------------------------------------------------\n",
      "Image                                              | Prediction      | Confidence\n",
      "--------------------------------------------------------------------------------\n",
      "h_1 10 referencia.JPG                              | Not Olive Fly   | 0.93\n",
      "h_1 11 referencia.JPG                              | Not Olive Fly   | 0.67\n",
      "h_1 12 referencia.JPG                              | Not Olive Fly   | 0.76\n",
      "h_1 13 referencia.JPG                              | Not Olive Fly   | 0.64\n",
      "h_1 14 referencia.JPG                              | Not Olive Fly   | 0.83\n",
      "h_1 16 referencia.JPG                              | Not Olive Fly   | 0.67\n",
      "h_1 17 referencia.JPG                              | Not Olive Fly   | 0.70\n",
      "h_1 21 referencia.JPG                              | Not Olive Fly   | 0.69\n",
      "h_1 22 referencia.JPG                              | Not Olive Fly   | 0.72\n",
      "moral_1 184 referencia.JPG                         | Olive Fly       | 0.96\n",
      "moral_1 205 referencia.JPG                         | Olive Fly       | 0.93\n",
      "moral_1 214 referencia.JPG                         | Olive Fly       | 0.73\n",
      "moral_1 245 referencia.JPG                         | Olive Fly       | 0.97\n",
      "moral_1 31 referencia.JPG                          | Olive Fly       | 0.85\n",
      "moral_1 40 referencia.JPG                          | Olive Fly       | 0.93\n",
      "moral_1 43 referencia.JPG                          | Olive Fly       | 0.94\n",
      "moral_1 44 referencia.JPG                          | Olive Fly       | 0.90\n",
      "moral_1 51 referencia.JPG                          | Olive Fly       | 0.73\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(detect_olive_fly(np_img))\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 127\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_images/h_1 14 referencia.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m np_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mstr\u001b[39m(img_path))])\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdetect_olive_fly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp_img\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[7], line 90\u001b[0m, in \u001b[0;36mdetect_olive_fly\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m model \u001b[38;5;241m=\u001b[39m OliveFlyDetector\u001b[38;5;241m.\u001b[39mload_model(\u001b[43mmodel_path\u001b[49m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Process all images\u001b[39;00m\n\u001b[0;32m     93\u001b[0m (prediction, probability) \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(image)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_folder):\n",
    "    \"\"\"Load training data with progress bar\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    paths = []\n",
    "    \n",
    "    data_path = Path(data_folder)\n",
    "    \n",
    "    # Count total files for progress bar\n",
    "    total_files = len(list((data_path / 'olive_fly').glob('*.jpg'))) + \\\n",
    "                  len(list((data_path / 'not_olive_fly').glob('*.jpg')))\n",
    "    \n",
    "    with tqdm(total=total_files, desc=\"Loading dataset\") as pbar:\n",
    "        # Load positive examples\n",
    "        for img_path in (data_path / 'olive_fly').glob('*.jpg'):\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                labels.append(1)\n",
    "                paths.append(str(img_path))\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Load negative examples\n",
    "        for img_path in (data_path / 'not_olive_fly').glob('*.jpg'):\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                labels.append(0)\n",
    "                paths.append(str(img_path))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return np.array(images), np.array(labels), paths\n",
    "\n",
    "def predict_images(test_folder, model_path):\n",
    "    \"\"\"Predict all images in a folder\"\"\"\n",
    "    detector = OliveFlyDetector.load_model(model_path)\n",
    "    \n",
    "    # Load test images\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    test_path = Path(test_folder)\n",
    "    \n",
    "    print(f\"\\nLoading test images from {test_folder}...\")\n",
    "    for img_path in tqdm(list(test_path.glob('*.jpg')), desc=\"Loading test images\"):\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            image_paths.append(str(img_path))\n",
    "    \n",
    "    if not images:\n",
    "        print(\"No images found in test folder!\")\n",
    "        return\n",
    "    \n",
    "    # Process all images\n",
    "    results = detector.predict_batch(images, image_paths)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Image':<50} | {'Prediction':<15} | {'Confidence':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for result in results:\n",
    "        img_name = Path(result['path']).name\n",
    "        print(f\"{img_name:<50} | {result['prediction']:<15} | {result['confidence']:.2f}\")\n",
    "\n",
    "\n",
    "def detect_olive_fly(image) -> bool:\n",
    "    MODEL_PATH = 'olive_fly_model.joblib'\n",
    "    DATA_FOLDER = 'training_data' # Folder with training data\n",
    "    \n",
    "    if not Path(MODEL_PATH).exists():\n",
    "        print(\"Training new model...\")\n",
    "        \n",
    "        # Load dataset\n",
    "        images, labels, _ = load_dataset(DATA_FOLDER)\n",
    "        if len(images) == 0:\n",
    "            print(f\"No training images found in {DATA_FOLDER}\")\n",
    "            print(\"Please create folders:\")\n",
    "            print(f\"  {DATA_FOLDER}/olive_fly/\")\n",
    "            print(f\"  {DATA_FOLDER}/not_olive_fly/\")\n",
    "            return\n",
    "        \n",
    "        # Train detector\n",
    "        detector = OliveFlyDetector()\n",
    "        detector.train(images, labels)\n",
    "        detector.save_model(MODEL_PATH)\n",
    "        print(f\"Model saved to {MODEL_PATH}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = OliveFlyDetector.load_model(model_path)\n",
    "    \n",
    "    # Process all images\n",
    "    (prediction, probability) = model.predict(image)\n",
    "    \n",
    "    return prediction == \"Olive Fly\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    MODEL_PATH = 'olive_fly_model.joblib'\n",
    "    DATA_FOLDER = 'training_data' # Folder with training data\n",
    "    TEST_FOLDER = 'test_images'  # Folder with images to classify\n",
    "    \n",
    "    if not Path(MODEL_PATH).exists():\n",
    "        print(\"Training new model...\")\n",
    "        \n",
    "        # Load dataset\n",
    "        images, labels, _ = load_dataset(DATA_FOLDER)\n",
    "        if len(images) == 0:\n",
    "            print(f\"No training images found in {DATA_FOLDER}\")\n",
    "            print(\"Please create folders:\")\n",
    "            print(f\"  {DATA_FOLDER}/olive_fly/\")\n",
    "            print(f\"  {DATA_FOLDER}/not_olive_fly/\")\n",
    "            return\n",
    "        \n",
    "        # Train detector\n",
    "        detector = OliveFlyDetector()\n",
    "        detector.train(images, labels)\n",
    "        detector.save_model(MODEL_PATH)\n",
    "        print(f\"Model saved to {MODEL_PATH}\")\n",
    "    \n",
    "    # Predict test images\n",
    "    predict_images(TEST_FOLDER, MODEL_PATH)\n",
    "\n",
    "    img_path = \"test_images/h_1 14 referencia.JPG\"\n",
    "    np_img = np.array([cv2.imread(str(img_path))])\n",
    "    print(detect_olive_fly(np_img))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e245856-ba09-439f-bce8-157a8c7fd50d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
